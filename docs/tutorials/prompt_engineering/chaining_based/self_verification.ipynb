{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e5f53e91f483eb",
   "metadata": {},
   "source": [
    "# Self Verification: Enhancing LLM Reasoning with Solution Evaluation\n",
    "\n",
    "This recipe demonstrates how to implement the Self Verification technique using Large Language Models (LLMs) with Mirascope. Self Verification is a solution evaluation technique that enhances an LLM's reasoning capabilities by generating multiple solutions and selecting the best one based on its ability to reconstruct the original problem.\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "<p class=\"admonition-title\">Mirascope Concepts Used</p>\n",
    "<ul>\n",
    "<li><a href=\"../../../../learn/prompts/\">Prompts</a></li>\n",
    "<li><a href=\"../../../../learn/calls/\">Calls</a></li>\n",
    "<li><a href=\"../../../../learn/dynamic_configuration/\">Dynamic Configuration</a></li>\n",
    "<li><a href=\"../../../../learn/response_models/\">Response Models</a></li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "<p class=\"admonition-title\">Background</p>\n",
    "<p>\n",
    "<a href=\"https://arxiv.org/pdf/2212.09561\">Self Verification</a> is not a direct prompt engineering technique, but rather a solution evaluation technique used to pick the best answer from a list of candidates. It can also be used to provide feedback for generating improved answers. The technique involves generating multiple Chain of Thought (CoT) solutions, identifying key information in the original prompt, creating masked versions of the prompt, and then evaluating each solution based on its ability to reconstruct the original prompt.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "## Self Verification Implementation\n",
    "\n",
    "Let's implement Self Verification using Mirascope:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa5a6c0c5f5e3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T02:46:33.563472Z",
     "start_time": "2024-10-01T02:41:37.614313Z"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "\n",
    "from mirascope.core import openai, prompt_template\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "@openai.call(model=\"gpt-4o-mini\", call_params={\"temperature\": random.uniform(0, 2)})\n",
    "async def zero_shot_cot(query: str) -> str:\n",
    "    return f\"Answer the following question, going through it step by step: {query}\"\n",
    "\n",
    "\n",
    "class ProblemInfo(BaseModel):\n",
    "    key_info: list[str] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"Key pieces of information needed to solve the problem.\n",
    "        Each key info is written as a short phrase.\"\"\",\n",
    "    )\n",
    "\n",
    "\n",
    "@openai.call(model=\"gpt-4o-mini\", response_model=ProblemInfo)\n",
    "@prompt_template(\n",
    "    \"\"\"\n",
    "    For the following question, identify the key pieces of information needed to answer the question.\n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    ")\n",
    "def identify_key_info(query: str): ...\n",
    "\n",
    "\n",
    "class MaskedPrompts(BaseModel):\n",
    "    masked_prompts: list[str] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"The original prompt with the specified key info replaced with X.\n",
    "        Example:\n",
    "        Prompt: Jane walked 6 miles today. It was a sunny day.\n",
    "        Key info: How many miles Jane walked\n",
    "        Masked Prompt: Jane walked X miles today. It was a sunny day.\"\"\",\n",
    "    )\n",
    "\n",
    "\n",
    "@openai.call(model=\"gpt-4o-mini\", response_model=MaskedPrompts)\n",
    "@prompt_template(\n",
    "    \"\"\"\n",
    "    Generate one masked copy of the prompt for each piece of key info.\n",
    "    The number of masked prompts you generate should be equal to the number of pieces of key info, and each masked prompt should mask one key piece of information.\n",
    "    Prompt: {query}\n",
    "    Key Info: {key_info}\n",
    "    \"\"\"\n",
    ")\n",
    "async def mask_prompt(query: str) -> openai.OpenAIDynamicConfig:\n",
    "    key_info = identify_key_info(query).key_info\n",
    "    return {\"computed_fields\": {\"key_info\": key_info}}\n",
    "\n",
    "\n",
    "@openai.call(model=\"gpt-4o-mini\")\n",
    "@prompt_template(\n",
    "    \"\"\"\n",
    "    SYSTEM:\n",
    "    You will be given a masked prompt with some value Xd out, and the solution to the original problem.\n",
    "    Return the prompt verbatim, but fill in the value for X according to what you see in the solution.\n",
    "\n",
    "    USER:\n",
    "    solution: {solution}\n",
    "    masked prompt: {masked_prompt}\n",
    "    \"\"\"\n",
    ")\n",
    "async def fill_in_value(solution: str, masked_prompt: str): ...\n",
    "\n",
    "\n",
    "class PromptComparison(BaseModel):\n",
    "    score: int = Field(\n",
    "        ...,\n",
    "        description=\"\"\"The number of variation prompts which are semantically equivalent to the original prompt. For numbers in the prompts, expect the same values and for quantitative words, only count semantically identical terms, such as two times = double.\"\"\",\n",
    "    )\n",
    "\n",
    "\n",
    "@openai.call(model=\"gpt-4o-mini\", response_model=PromptComparison)\n",
    "@prompt_template(\n",
    "    \"\"\"\n",
    "    SYSTEM:\n",
    "    You will be given an original prompt and some variations of it.\n",
    "    Return the number of variations which are semantically identical.\n",
    "\n",
    "    USER:\n",
    "    Original Prompt:\n",
    "    {query}\n",
    "    Variations:\n",
    "    {numbered_variations}\n",
    "    \"\"\"\n",
    ")\n",
    "def score_variations(\n",
    "    query: str, filled_in_prompts: list[str]\n",
    ") -> openai.OpenAIDynamicConfig:\n",
    "    return {\n",
    "        \"computed_fields\": {\n",
    "            \"numbered_variations\": [\n",
    "                f\"{i+1}. {_}\" for i, _ in enumerate(filled_in_prompts)\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "async def evaluate_solution(query: str, solution: str, masked_prompts: list[str]):\n",
    "    tasks = [fill_in_value(solution, masked_prompt) for masked_prompt in masked_prompts]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    filled_in_prompts = [response.content for response in responses]\n",
    "    score = score_variations(query, filled_in_prompts).score\n",
    "    return score\n",
    "\n",
    "\n",
    "async def self_verify(query: str, num_solutions: int) -> None:\n",
    "    tasks = [mask_prompt(query)]\n",
    "    tasks += [zero_shot_cot(query) for _ in range(num_solutions)]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    masked_prompts = responses[0].masked_prompts\n",
    "    cot_responses = [response.content for response in responses[1:]]\n",
    "    tasks = [\n",
    "        evaluate_solution(query, solution, masked_prompts) for solution in cot_responses\n",
    "    ]\n",
    "    scores = await asyncio.gather(*tasks)\n",
    "    print(f\"MAX SCORE:{len(masked_prompts)}\")\n",
    "    for cot_solution, score in zip(cot_responses, scores):\n",
    "        print(f\"Solution:\\n{cot_solution}\\nScore:\\n{score}\\n\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "query = \"\"\"Tim wanted to make lemonade for a pool party. For a gallon of lemonade, \\\n",
    "his recipe called for 1 cup of fresh lemon juice. He found that 6 lemons would yield \\\n",
    "1 cup of juice. He figured he would need to make 4 gallons of lemonade for the \\\n",
    "party. His best friend Allen asked if Tim could make an extra gallon for him that \\\n",
    "was twice as tart as the other gallons. How many lemons will Tim need?\"\"\"\n",
    "\n",
    "await self_verify(query, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21172cac61a1cb",
   "metadata": {},
   "source": [
    "This implementation demonstrates the core components of Self Verification:\n",
    "\n",
    "1. Generating multiple Chain of Thought solutions using `zero_shot_cot`.\n",
    "2. Identifying key information in the prompt with `identify_key_info`.\n",
    "3. Creating masked versions of the prompt using `mask_prompt`.\n",
    "4. Evaluating solutions by reconstructing the original prompt with `fill_in_value` and `score_variations`.\n",
    "\n",
    "## Benefits and Considerations\n",
    "\n",
    "The Self Verification technique offers several advantages:\n",
    "\n",
    "1. Improved accuracy: By generating multiple solutions and evaluating them, the technique can identify the most accurate and complete answer.\n",
    "2. Robustness: Self Verification helps mitigate the impact of occasional errors or inconsistencies in LLM outputs.\n",
    "3. Transparency: The scoring process provides insight into how well each solution captures the key information from the original prompt.\n",
    "4. Adaptability: The technique can be applied to a wide range of problem types and domains.\n",
    "\n",
    "When implementing Self Verification, consider the following:\n",
    "\n",
    "- Computational cost: Generating and evaluating multiple solutions requires more API calls and processing time compared to single-solution approaches.\n",
    "- Prompt engineering: The effectiveness of the technique relies on well-crafted prompts for generating solutions and identifying key information.\n",
    "- Scoring mechanism: The current implementation uses a simple count of correctly filled masked prompts. More sophisticated scoring methods could be developed for specific use cases.\n",
    "- Trade-offs between quantity and quality: Increasing the number of generated solutions may improve the chances of finding a high-quality answer but also increases computational overhead.\n",
    "- Integration with other techniques: Self Verification can be combined with other prompt engineering methods like Chain of Thought or Self-Consistency for potentially even better results.\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "<p class=\"admonition-title\">Additional Real-World Applications</p>\n",
    "<ul>\n",
    "<li><b>Complex problem solving</b>: Use Self Verification for multi-step problems in fields like engineering or scientific research.</li>\n",
    "<li><b>Content generation</b>: Improve the quality and accuracy of automatically generated reports or articles.</li>\n",
    "<li><b>Decision support systems</b>: Enhance the reliability of AI-generated recommendations in high-stakes scenarios.</li>\n",
    "<li><b>Educational tools</b>: Create more accurate and comprehensive explanations for students in various subjects.</li>\n",
    "<li><b>Data analysis</b>: Verify and improve the interpretation of complex datasets in fields like finance or healthcare.</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "When adapting this recipe to your specific use-case, consider:\n",
    "\n",
    "- Tailoring the key information extraction process to your domain for better performance.\n",
    "- Experimenting with different scoring mechanisms that align with your specific accuracy requirements.\n",
    "- Implementing a feedback loop to continuously improve the quality of the generated solutions.\n",
    "- Combining Self Verification with other techniques like Chain of Thought or Self-Consistency for even more powerful reasoning capabilities.\n",
    "\n",
    "By leveraging Mirascope's `call` decorator, response models, and dynamic configuration, you can easily implement and customize the Self Verification technique to enhance your LLM's reasoning capabilities across a wide range of applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
