{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Tools and Agents in Mirascope\n",
    "\n",
    "This notebook provides a detailed introduction to using Tools and implementing Agents in Mirascope. We'll use the WebSearchAgent as our primary example to demonstrate these concepts.\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setting Up the Environment](#Setting-Up-the-Environment)\n",
    "3. [Building a Basic Chatbot](#Building-a-Basic-Chatbot)\n",
    "4. [Understanding Tools in Mirascope](#Understanding-Tools-in-Mirascope)\n",
    "5. [Creating Custom Tools](#Creating-Custom-Tools)\n",
    "6. [Implementing the WebSearchAgent](#Implementing-the-WebSearchAgent)\n",
    "7. [Running and Testing the Agent](#Running-and-Testing-the-Agent)\n",
    "8. [Advanced Concepts and Best Practices](#Advanced-Concepts-and-Best-Practices)\n",
    "9. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Tools and Agents are two key concepts in building advanced AI systems, particularly those involving Large Language Models (LLMs):\n",
    "\n",
    "- **Tools**: Functions that extend the capabilities of LLMs, allowing them to perform specific tasks or access external data.\n",
    "- **Agents**: Autonomous or semi-autonomous entities that use LLMs and Tools to perform complex tasks or interact with users.\n",
    "\n",
    "In this notebook, we'll explore how to create and use Tools, and how to implement an Agent using the WebSearchAgent as our example. We'll be using Mirascope, a toolkit that simplifies the process of building LLM-powered applications with tools and agents.\n",
    "\n",
    "For more detailed information on these concepts, refer to the following Mirascope documentation:\n",
    "\n",
    "- [Tools documentation](https://docs.mirascope.io/latest/learn/tools/)\n",
    "- [Agents documentation](https://docs.mirascope.io/latest/learn/agents/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Environment\n",
    "\n",
    "First, let's set up our environment by installing Mirascope and other necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"mirascope[openai]\" duckduckgo_search beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "For more information on setting up Mirascope and its dependencies, see the [Mirascope installation guide](https://docs.mirascope.io/latest/get-started/)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Basic Chatbot\n",
    "\n",
    "Before we dive into Tools and Agents, let's start by building a basic chatbot. This will help us understand the fundamental concepts of state management and conversation flow in Mirascope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T01:39:45.815456Z",
     "start_time": "2024-09-12T01:39:20.717191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Good morning!\n",
      "Chatbot: Good morning! How are you today?\n",
      "User: I'm doing well, thank you! How about you?\n",
      "Chatbot: I’m doing great, thanks for asking! Is there anything specific you’d like to chat about today?\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "from mirascope.core import BaseMessageParam, openai, prompt_template\n",
    "\n",
    "\n",
    "class BasicChatbot(BaseModel):\n",
    "    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n",
    "\n",
    "    @openai.call(model=\"gpt-4o-mini\")\n",
    "    @prompt_template(\n",
    "        \"\"\"\n",
    "        SYSTEM: You are a friendly chatbot assistant. Engage in a conversation with the user.\n",
    "        MESSAGES: {self.messages}\n",
    "        USER: {user_input}\n",
    "        \"\"\"\n",
    "    )\n",
    "    async def chat(self, user_input: str): ...\n",
    "\n",
    "    async def run(self):\n",
    "        while True:\n",
    "            user_input = input(\"User: \")\n",
    "            if user_input.lower() == \"exit\":\n",
    "                break\n",
    "            response = await self.chat(user_input)\n",
    "            print(f\"User: {user_input}\")\n",
    "            print(f\"Chatbot: {response.content}\")\n",
    "            self.messages.append(response.user_message_param)\n",
    "            self.messages.append(response.message_param)\n",
    "\n",
    "\n",
    "# Usage\n",
    "chatbot = BasicChatbot()\n",
    "# Run the chatbot in a Jupyter notebook\n",
    "await chatbot.run()\n",
    "\n",
    "# Run the chatbot in a Python script\n",
    "# import asyncio\n",
    "# asyncio.run(chatbot.run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this basic chatbot implementation, we've created a `BasicChatbot` class that maintains a conversation history (`messages`) and uses Mirascope's `@openai.call` and `@prompt_template` decorators to interact with the LLM. Note that the `chat` method can directly access `self.messages` in the template, allowing for easy integration of the conversation history into the prompt.\n",
    "\n",
    "The `chat` method is where the LLM interaction occurs. It uses the conversation history and the current user input to generate a response. The `run` method manages the main conversation loop, updating the conversation history after each interaction.\n",
    "\n",
    "This sets the foundation for more complex agents that we'll build upon in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Tools in Mirascope\n",
    "\n",
    "Tools in Mirascope are functions that extend the capabilities of LLMs. They allow the LLM to perform specific tasks or access external data. Tools are typically used to:\n",
    "\n",
    "1. Retrieve information from external sources\n",
    "2. Perform calculations or data processing\n",
    "3. Interact with APIs or databases\n",
    "4. Execute specific actions based on the LLM's decisions\n",
    "\n",
    "Let's start by creating a Tool that extracts content from a webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T01:39:48.637129Z",
     "start_time": "2024-09-12T01:39:48.547595Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_content(url: str) -> str:\n",
    "    \"\"\"Extract the main content from a webpage.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the webpage to extract the content from.\n",
    "\n",
    "    Returns:\n",
    "        The extracted content as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        unwanted_tags = [\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]\n",
    "        for tag in unwanted_tags:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        main_content = (\n",
    "            soup.find(\"main\")\n",
    "            or soup.find(\"article\")\n",
    "            or soup.find(\"div\", class_=re.compile(\"content|main\"))\n",
    "        )\n",
    "        if main_content:\n",
    "            text = main_content.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        return \"\\n\".join(line for line in lines if line)\n",
    "    except Exception as e:\n",
    "        return f\"{type(e)}: Failed to extract content from URL {url}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `extract_content` function is a Tool that takes a URL as input and returns the main content of the webpage as a string. It uses [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) to parse the HTML and extract the relevant text content.\n",
    "\n",
    "When this function is passed to a Mirascope `call` decorator, it is automatically converted into a `BaseTool` class. This conversion process generates a schema that the LLM API uses to understand and interact with the tool.\n",
    "\n",
    "Let's take a look at what this schema looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T01:39:50.790282Z",
     "start_time": "2024-09-12T01:39:50.757565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'function': {'name': 'extract_content', 'description': 'Extract the main content from a webpage.\\n\\nArgs:\\n    url: The URL of the webpage to extract the content from.\\n\\nReturns:\\n    The extracted content as a string.', 'parameters': {'properties': {'url': {'description': 'The URL of the webpage to extract the content from.', 'type': 'string'}}, 'required': ['url'], 'type': 'object'}}, 'type': 'function'}\n"
     ]
    }
   ],
   "source": [
    "from mirascope.core.openai import OpenAITool\n",
    "\n",
    "tool_type = OpenAITool.type_from_fn(extract_content)\n",
    "print(tool_type.tool_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This schema provides the LLM with information about the tool's name, description, and expected input parameters. The LLM uses this information to determine when and how to use the tool during its reasoning process.\n",
    "\n",
    "For more details on implementing and using Tools in Mirascope, see the [Tools documentation](https://docs.mirascope.io/latest/learn/tools/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tools With Access To Agent State\n",
    "\n",
    "Now, let's create a more complex Tool that performs web searches using the DuckDuckGo search engine. We'll implement this tool as a method within our agent class to demonstrate how tools can access the agent's state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T01:39:54.329920Z",
     "start_time": "2024-09-12T01:39:54.284790Z"
    }
   },
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "\n",
    "\n",
    "class WebSearchAgentBase(BaseModel):\n",
    "    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n",
    "    search_history: list[str] = []\n",
    "    max_results_per_query: int = 2\n",
    "\n",
    "    def web_search(self, queries: list[str]) -> str:\n",
    "        \"\"\"Performs web searches for given queries and returns URLs.\n",
    "\n",
    "        Args:\n",
    "            queries: List of search queries.\n",
    "\n",
    "        Returns:\n",
    "            str: Newline-separated URLs from search results or error messages.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If web search fails entirely.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            urls = []\n",
    "            for query in queries:\n",
    "                results = DDGS(proxies=None).text(\n",
    "                    query, max_results=self.max_results_per_query\n",
    "                )\n",
    "                for result in results:\n",
    "                    link = result[\"href\"]\n",
    "                    try:\n",
    "                        urls.append(link)\n",
    "                    except Exception as e:\n",
    "                        urls.append(\n",
    "                            f\"{type(e)}: Failed to parse content from URL {link}\"\n",
    "                        )\n",
    "            self.search_history.extend(queries)\n",
    "            return \"\\n\\n\".join(urls)\n",
    "        except Exception as e:\n",
    "            return f\"{type(e)}: Failed to search the web for text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `web_search` method is a more complex Tool that takes a list of search queries and returns a string of newline-separated URLs from the search results. It uses the DuckDuckGo search API to perform the searches and updates the agent's `search_history`. Luckily, the DuckDuckGo search API does not require an API key, making it easy to use in this example.\n",
    "\n",
    "By implementing this tool as a method within our `WebSearchAgent` class, we can access and update the agent's state (like `search_history`) directly. This approach allows for more integrated and stateful tool usage within our agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementing the WebSearchAgent\n",
    "\n",
    "Now that we have our custom tools, let's implement the full WebSearchAgent by adding the LLM interaction and main execution flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T01:39:57.400716Z",
     "start_time": "2024-09-12T01:39:57.390902Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class WebSearchAgent(WebSearchAgentBase):\n",
    "    @openai.call(model=\"gpt-4o-mini\", stream=True)\n",
    "    @prompt_template(\n",
    "        \"\"\"\n",
    "        SYSTEM:\n",
    "        You are an expert web searcher. Your task is to answer the user's question using the provided tools.\n",
    "        The current date is {current_date}.\n",
    "\n",
    "        You have access to the following tools:\n",
    "        - `web_search`: Search the web when the user asks a question. Follow these steps for EVERY web search query:\n",
    "            1. There is a previous search context: {self.search_history}\n",
    "            2. There is the current user query: {question}\n",
    "            3. Given the previous search context, generate multiple search queries that explores whether the new query might be related to or connected with the context of the current user query. \n",
    "                Even if the connection isn't immediately clear, consider how they might be related.\n",
    "        - `extract_content`: Parse the content of a webpage.\n",
    "\n",
    "        When calling the `web_search` tool, the `body` is simply the body of the search\n",
    "        result. You MUST then call the `extract_content` tool to get the actual content\n",
    "        of the webpage. It is up to you to determine which search results to parse.\n",
    "\n",
    "        Once you have gathered all of the information you need, generate a writeup that\n",
    "        strikes the right balance between brevity and completeness based on the context of the user's query.\n",
    "\n",
    "        MESSAGES: {self.messages}\n",
    "        USER: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "    async def _stream(self, question: str) -> openai.OpenAIDynamicConfig:\n",
    "        return {\n",
    "            \"tools\": [self.web_search, extract_content],\n",
    "            \"computed_fields\": {\n",
    "                \"current_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            },\n",
    "        }\n",
    "\n",
    "    async def _step(self, question: str):\n",
    "        response = await self._stream(question)\n",
    "        tools_and_outputs = []\n",
    "        async for chunk, tool in response:\n",
    "            if tool:\n",
    "                print(f\"using {tool._name()} tool with args: {tool.args}\")\n",
    "                tools_and_outputs.append((tool, tool.call()))\n",
    "            else:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "        if response.user_message_param:\n",
    "            self.messages.append(response.user_message_param)\n",
    "        self.messages.append(response.message_param)\n",
    "        if tools_and_outputs:\n",
    "            self.messages += response.tool_message_params(tools_and_outputs)\n",
    "            await self._step(\"\")\n",
    "\n",
    "    async def run(self):\n",
    "        while True:\n",
    "            question = input(\"(User): \")\n",
    "            if question == \"exit\":\n",
    "                break\n",
    "            print(f\"(User): {question}\")\n",
    "            print(\"(Assistant): \", end=\"\", flush=True)\n",
    "            await self._step(question)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation includes:\n",
    "\n",
    "1. The `_stream` method, which sets up the LLM call with the necessary tools and computed fields.\n",
    "2. The `_step` method, which processes the LLM response, handles tool calls, and updates the conversation history.\n",
    "3. The `run` method, which implements the main interaction loop for the agent.\n",
    "\n",
    "The `@openai.call` and `@prompt_template` decorators are used to set up the LLM interaction and define the prompt for the agent. Note that we've explicitly specified the available tools and their usage instructions in the system prompt. We have found that this improves the performance and reliability of tool usage by the LLM.\n",
    "\n",
    "Note how we're passing `self.web_search` as a tool, which allows the LLM to use our custom web search method that has access to the agent's state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Running and Testing the Agent\n",
    "\n",
    "Now that we have implemented our WebSearchAgent, let's run and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T01:40:40.019841Z",
     "start_time": "2024-09-12T01:40:02.686570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(User): large language models\n",
      "(Assistant): using web_search tool with args: {'queries': ['overview of large language models', 'applications of large language models', 'recent advancements in large language models', 'large language models in artificial intelligence', 'challenges in large language models']}\n",
      "using extract_content tool with args: {'url': 'https://blogs.nvidia.com/blog/what-are-large-language-models-used-for/'}\n",
      "using extract_content tool with args: {'url': 'https://www.techopedia.com/12-practical-large-language-model-llm-applications'}\n",
      "using extract_content tool with args: {'url': 'https://www.computerworld.com/article/1627101/what-are-large-language-models-and-how-are-they-used-in-generative-ai.html'}\n",
      "using extract_content tool with args: {'url': 'https://arxiv.org/abs/2307.06435'}\n",
      "Large Language Models (LLMs) represent a significant advancement in artificial intelligence, particularly in natural language processing (NLP). These models are powerful deep learning algorithms capable of understanding and generating human-like text, thanks to their training on vast datasets comprising internet text, books, and other written content.\n",
      "\n",
      "### Overview of Large Language Models\n",
      "- **Definition**: LLMs are types of AI that process and predict text based on continuous exposure to massive datasets. They use a neural network architecture, often based on transformer models, to recognize patterns in language.\n",
      "- **Functionality**: These models operate primarily as next-word prediction engines, meaning they analyze the input they receive and predict what comes next based on their training. For example, models like OpenAI's GPT-3 and GPT-4 can generate coherent and contextually relevant sentences from the prompts provided.\n",
      "  \n",
      "### Applications of Large Language Models\n",
      "LLMs have a broad range of applications across various fields, including:\n",
      "1. **Conversational Agents**: They serve as the backbone for chatbots like ChatGPT, helping to automate customer service interactions and support.\n",
      "2. **Content Generation**: Businesses utilize LLMs for writing assistance, generating marketing copy, drafting emails, and even creative tasks like composing poetry or music.\n",
      "3. **Code Generation**: Tools like GitHub Copilot leverage LLMs to assist developers in writing and debugging code.\n",
      "4. **Healthcare**: LLMs analyze patient data and scientific literature, aiding in research and diagnostics.\n",
      "5. **Education**: They power tutoring systems that can tailor responses to student inquiries.\n",
      "\n",
      "### Recent Advancements\n",
      "Recent work has focused on improving LLM capabilities through:\n",
      "- **Fine-tuning**: Adjusting models to perform better in specific fields or tasks using targeted datasets.\n",
      "- **Multi-modal Learning**: Developing models that can process and integrate information from multiple types of input, such as text and images.\n",
      "- **Efficiency Improvements**: Innovations aim at reducing the computational resources required, making it feasible to run LLMs on less powerful hardware.\n",
      "\n",
      "### Challenges and Considerations\n",
      "Despite their capabilities, LLMs face several challenges:\n",
      "- **Bias and Ethics**: These models can inherit biases present in their training data, leading to skewed or inappropriate outputs. There is ongoing research to mitigate these biases and promote ethical AI use.\n",
      "- **Resource Intensity**: Training LLMs requires significant computational power and data. The cost associated with this can be prohibitive, limiting access to advanced models.\n",
      "- **Privacy Concerns**: Ensuring the confidentiality of user data during model training and operation presents ongoing challenges, especially in sectors like healthcare and finance.\n",
      "\n",
      "### Future Directions\n",
      "The future of LLMs likely includes:\n",
      "- **Custom Models**: Organizations are increasingly interested in designing customized LLMs optimized for their specific needs, which can improve performance and reduce costs.\n",
      "- **Broader Accessibility**: Efforts are being made to democratize access to LLM technology, enabling more organizations, including those with fewer resources, to benefit from this technology.\n",
      "\n",
      "In conclusion, large language models are a transformative technology in AI, with a growing impact across industries. Continuous research and development are essential to address their challenges and unlock their full potential effectively.\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    web_assistant = WebSearchAgent()\n",
    "    await web_assistant.run()\n",
    "\n",
    "\n",
    "# Run main in a jupyter notebook\n",
    "await main()\n",
    "\n",
    "# Run main in a python script\n",
    "# import asyncio\n",
    "# asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the WebSearchAgent, run the code above and start interacting with it by typing your questions. The agent will use web searches and content extraction to provide answers. Type 'exit' to end the interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Concepts and Best Practices\n",
    "\n",
    "When working with Tools and Agents in Mirascope, consider the following best practices:\n",
    "\n",
    "1. **Error Handling**: Implement robust error handling in your Tools and Agent implementation.\n",
    "2. **Rate Limiting**: Be mindful of rate limits when using external APIs in your Tools.\n",
    "3. **Caching**: Implement caching mechanisms to improve performance and reduce API calls.\n",
    "4. **Testing**: Write unit tests for your Tools and integration tests for your Agents.\n",
    "5. **Modularity**: Design your Tools and Agents to be modular and reusable.\n",
    "6. **Security**: Be cautious about the information you expose through your Tools and Agents.\n",
    "7. **Logging**: Implement logging to track the behavior and performance of your Agents.\n",
    "\n",
    "For more advanced usage, you can explore concepts like:\n",
    "\n",
    "- Multi-agent systems\n",
    "- Tool chaining and composition\n",
    "- Dynamic tool selection\n",
    "- Memory and state management for long-running agents\n",
    "\n",
    "For more techniques and best practices in using Mirascope, refer to the following documentation:\n",
    "\n",
    "- [Tools](https://docs.mirascope.io/latest/learn/tools/)\n",
    "- [Agents](https://docs.mirascope.io/latest/learn/agents/)\n",
    "- [Dynamic Configuration](https://docs.mirascope.io/latest/learn/dynamic_configuration/)\n",
    "- [Chaining](https://docs.mirascope.io/latest/learn/chaining/)\n",
    "- [Streams](https://docs.mirascope.io/latest/learn/streams/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've explored the basics of creating Tools and implementing Agents in Mirascope. We've built a WebSearchAgent that can perform web searches, extract content from webpages, and use an LLM to generate responses based on the gathered information.\n",
    "\n",
    "This example demonstrates the power and flexibility of Mirascope in building AI applications that combine LLMs with custom tools and logic. As you continue to work with Mirascope, you'll discover more advanced features and patterns that can help you build even more sophisticated AI agents and applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
