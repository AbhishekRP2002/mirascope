{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Tools and Agents in Mirascope\n",
    "\n",
    "This notebook provides a detailed introduction to using Tools and implementing Agents in Mirascope. We'll use the WebSearchAgent as our primary example to demonstrate these concepts.\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setting Up the Environment](#Setting-Up-the-Environment)\n",
    "3. [Building a Basic Chatbot](#Building-a-Basic-Chatbot)\n",
    "4. [Understanding Tools in Mirascope](#Understanding-Tools-in-Mirascope)\n",
    "5. [Creating Custom Tools](#Creating-Custom-Tools)\n",
    "6. [Implementing the WebSearchAgent](#Implementing-the-WebSearchAgent)\n",
    "7. [Running and Testing the Agent](#Running-and-Testing-the-Agent)\n",
    "8. [Advanced Concepts and Best Practices](#Advanced-Concepts-and-Best-Practices)\n",
    "9. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Tools and Agents are two key concepts in building advanced AI systems, particularly those involving Large Language Models (LLMs):\n",
    "\n",
    "- **Tools**: Functions that extend the capabilities of LLMs, allowing them to perform specific tasks or access external data.\n",
    "- **Agents**: Autonomous or semi-autonomous entities that use LLMs and Tools to perform complex tasks or interact with users.\n",
    "\n",
    "In this notebook, we'll explore how to create and use Tools, and how to implement an Agent using the WebSearchAgent as our example. We'll be using Mirascope, a toolkit that simplifies the process of building LLM-powered applications with tools and agents.\n",
    "\n",
    "For more detailed information on these concepts, refer to the following Mirascope documentation:\n",
    "\n",
    "- [Tools documentation](https://docs.mirascope.io/latest/learn/tools/)\n",
    "- [Agents documentation](https://docs.mirascope.io/latest/learn/agents/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Environment\n",
    "\n",
    "First, let's set up our environment by installing Mirascope and other necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"mirascope[openai]\" duckduckgo_search beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "For more information on setting up Mirascope and its dependencies, see the [Mirascope installation guide](https://docs.mirascope.io/latest/get-started/)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Basic Chatbot\n",
    "\n",
    "Before we dive into Tools and Agents, let's start by building a basic chatbot. This will help us understand the fundamental concepts of state management and conversation flow in Mirascope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:09:03.042870Z",
     "start_time": "2024-09-11T08:08:20.226209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Good morning!\n",
      "Chatbot: Good morning! How are you today?\n",
      "User: I'm doing well, thank you! How about you?\n",
      "Chatbot: I’m doing great, thanks for asking! What’s on your mind today?\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "from mirascope.core import BaseMessageParam, openai, prompt_template\n",
    "\n",
    "\n",
    "class BasicChatbot(BaseModel):\n",
    "    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n",
    "\n",
    "    @openai.call(model=\"gpt-4o-mini\")\n",
    "    @prompt_template(\n",
    "        \"\"\"\n",
    "        SYSTEM: You are a friendly chatbot assistant. Engage in a conversation with the user.\n",
    "        \n",
    "        MESSAGES: {self.messages}\n",
    "        USER: {user_input}\n",
    "        \"\"\"\n",
    "    )\n",
    "    def chat(self, user_input: str): ...\n",
    "\n",
    "    async def run(self):\n",
    "        while True:\n",
    "            user_input = input(\"User: \")\n",
    "            if user_input.lower() == \"exit\":\n",
    "                break\n",
    "            response = self.chat(user_input)\n",
    "            print(f\"User: {user_input}\")\n",
    "            print(f\"Chatbot: {response.content}\")\n",
    "            self.messages.append(response.user_message_param)\n",
    "            self.messages.append(response.message_param)\n",
    "\n",
    "\n",
    "# Usage\n",
    "chatbot = BasicChatbot()\n",
    "# Run the chatbot in a Jupyter notebook\n",
    "await chatbot.run()\n",
    "\n",
    "# Run the chatbot in a Python script\n",
    "# import asyncio\n",
    "# asyncio.run(chatbot.run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this basic chatbot implementation, we've created a `BasicChatbot` class that maintains a conversation history (`messages`) and uses Mirascope's `@openai.call` and `@prompt_template` decorators to interact with the LLM.\n",
    "\n",
    "The `chat` method is where the LLM interaction occurs. It uses the conversation history and the current user input to generate a response. The `run` method manages the main conversation loop, updating the conversation history after each interaction.\n",
    "\n",
    "This sets the foundation for more complex agents that we'll build upon in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Tools in Mirascope\n",
    "\n",
    "Tools in Mirascope are functions that extend the capabilities of LLMs. They allow the LLM to perform specific tasks or access external data. Tools are typically used to:\n",
    "\n",
    "1. Retrieve information from external sources\n",
    "2. Perform calculations or data processing\n",
    "3. Interact with APIs or databases\n",
    "4. Execute specific actions based on the LLM's decisions\n",
    "\n",
    "Let's start by creating a simple Tool that extracts content from a webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:09:32.534799Z",
     "start_time": "2024-09-11T08:09:32.424806Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_content(url: str) -> str:\n",
    "    \"\"\"Extract the main content from a webpage.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the webpage to extract the content from.\n",
    "\n",
    "    Returns:\n",
    "        The extracted content as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        unwanted_tags = [\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]\n",
    "        for tag in unwanted_tags:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        main_content = (\n",
    "            soup.find(\"main\")\n",
    "            or soup.find(\"article\")\n",
    "            or soup.find(\"div\", class_=re.compile(\"content|main\"))\n",
    "        )\n",
    "        if main_content:\n",
    "            text = main_content.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        return \"\\n\".join(line for line in lines if line)\n",
    "    except Exception as e:\n",
    "        return f\"{type(e)}: Failed to extract content from URL {url}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `extract_content` function is a Tool that takes a URL as input and returns the main content of the webpage as a string. It uses BeautifulSoup to parse the HTML and extract the relevant text content.\n",
    "\n",
    "When this function is passed to a Mirascope `call` decorator, it is automatically converted into a `BaseTool` class. This conversion process generates a schema that the LLM API uses to understand and interact with the tool.\n",
    "\n",
    "Let's take a look at what this schema looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:09:38.793246Z",
     "start_time": "2024-09-11T08:09:38.778077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'function': {'name': 'extract_content', 'description': 'Extract the main content from a webpage.\\n\\nArgs:\\n    url: The URL of the webpage to extract the content from.\\n\\nReturns:\\n    The extracted content as a string.', 'parameters': {'properties': {'url': {'description': 'The URL of the webpage to extract the content from.', 'type': 'string'}}, 'required': ['url'], 'type': 'object'}}, 'type': 'function'}\n"
     ]
    }
   ],
   "source": [
    "from mirascope.core.openai import OpenAITool\n",
    "\n",
    "tool_type = OpenAITool.type_from_fn(extract_content)\n",
    "print(tool_type.tool_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This schema provides the LLM with information about the tool's name, description, and expected input parameters. The LLM uses this information to determine when and how to use the tool during its reasoning process.\n",
    "\n",
    "For more details on implementing and using Tools in Mirascope, see the [Tools documentation](https://docs.mirascope.io/latest/learn/tools/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating Custom Tools\n",
    "\n",
    "Now, let's create a more complex Tool that performs web searches using the DuckDuckGo search engine. We'll implement this tool as a method within our agent class to demonstrate how tools can access the agent's state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:22:28.087233Z",
     "start_time": "2024-09-11T08:22:28.030706Z"
    }
   },
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "\n",
    "\n",
    "class WebSearchAgentBase(BaseModel):\n",
    "    messages: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n",
    "    search_history: list[str] = []\n",
    "    max_results_per_query: int = 2\n",
    "\n",
    "    def web_search(self, queries: list[str]) -> str:\n",
    "        \"\"\"Performs web searches for given queries and returns URLs.\n",
    "\n",
    "        Args:\n",
    "            queries: List of search queries.\n",
    "\n",
    "        Returns:\n",
    "            str: Newline-separated URLs from search results or error messages.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If web search fails entirely.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            urls = []\n",
    "            for query in queries:\n",
    "                results = DDGS(proxies=None).text(\n",
    "                    query, max_results=self.max_results_per_query\n",
    "                )\n",
    "                for result in results:\n",
    "                    link = result[\"href\"]\n",
    "                    try:\n",
    "                        urls.append(link)\n",
    "                    except Exception as e:\n",
    "                        urls.append(\n",
    "                            f\"{type(e)}: Failed to parse content from URL {link}\"\n",
    "                        )\n",
    "            self.search_history.extend(queries)\n",
    "            return \"\\n\\n\".join(urls)\n",
    "        except Exception as e:\n",
    "            return f\"{type(e)}: Failed to search the web for text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `web_search` method is a more complex Tool that takes a list of search queries and returns a string of newline-separated URLs from the search results. It uses the DuckDuckGo search API to perform the searches and updates the agent's `search_history`.\n",
    "\n",
    "By implementing this tool as a method within our `WebSearchAgent` class, we can access and update the agent's state (like `search_history`) directly. This approach allows for more integrated and stateful tool usage within our agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementing the WebSearchAgent\n",
    "\n",
    "Now that we have our custom tools, let's implement the full WebSearchAgent by adding the LLM interaction and main execution flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:22:32.647699Z",
     "start_time": "2024-09-11T08:22:32.544782Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class WebSearchAgent(WebSearchAgentBase):\n",
    "    @openai.call(model=\"gpt-4o-mini\", stream=True)\n",
    "    @prompt_template(\n",
    "        \"\"\"\n",
    "        SYSTEM:\n",
    "        You are an expert web searcher. Your task is to answer the user's question using the provided tools.\n",
    "        The current date is {current_date}.\n",
    "\n",
    "        You have access to the following tools:\n",
    "        - `web_search`: Search the web when the user asks a question. Follow these steps for EVERY web search query:\n",
    "            1. There is a previous search context: {self.search_history}\n",
    "            2. There is the current user query: {question}\n",
    "            3. Given the previous search context, generate multiple search queries that explores whether the new query might be related to or connected with the context of the current user query. \n",
    "                Even if the connection isn't immediately clear, consider how they might be related.\n",
    "        - `extract_content`: Parse the content of a webpage.\n",
    "\n",
    "        When calling the `web_search` tool, the `body` is simply the body of the search\n",
    "        result. You MUST then call the `extract_content` tool to get the actual content\n",
    "        of the webpage. It is up to you to determine which search results to parse.\n",
    "\n",
    "        Once you have gathered all of the information you need, generate a writeup that\n",
    "        strikes the right balance between brevity and completeness based on the context of the user's query.\n",
    "\n",
    "        MESSAGES: {self.messages}\n",
    "        USER: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "    async def _stream(self, question: str) -> openai.OpenAIDynamicConfig:\n",
    "        return {\n",
    "            \"tools\": [self.web_search, extract_content],\n",
    "            \"computed_fields\": {\n",
    "                \"current_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            },\n",
    "        }\n",
    "\n",
    "    async def _step(self, question: str):\n",
    "        response = await self._stream(question)\n",
    "        tools_and_outputs = []\n",
    "        async for chunk, tool in response:\n",
    "            if tool:\n",
    "                print(f\"using {tool._name()} tool with args: {tool.args}\")\n",
    "                tools_and_outputs.append((tool, tool.call()))\n",
    "            else:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "        if response.user_message_param:\n",
    "            self.messages.append(response.user_message_param)\n",
    "        self.messages.append(response.message_param)\n",
    "        if tools_and_outputs:\n",
    "            self.messages += response.tool_message_params(tools_and_outputs)\n",
    "            await self._step(\"\")\n",
    "\n",
    "    async def run(self):\n",
    "        while True:\n",
    "            question = input(\"(User): \")\n",
    "            if question == \"exit\":\n",
    "                break\n",
    "            print(\"(Assistant): \", end=\"\", flush=True)\n",
    "            await self._step(question)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation includes:\n",
    "\n",
    "1. The `_stream` method, which sets up the LLM call with the necessary tools and computed fields.\n",
    "2. The `_step` method, which processes the LLM response, handles tool calls, and updates the conversation history.\n",
    "3. The `run` method, which implements the main interaction loop for the agent.\n",
    "\n",
    "The `@openai.call` and `@prompt_template` decorators are used to set up the LLM interaction and define the prompt for the agent.\n",
    "\n",
    "Note how we're passing `self.web_search` as a tool, which allows the LLM to use our custom web search method that has access to the agent's state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Running and Testing the Agent\n",
    "\n",
    "Now that we have implemented our WebSearchAgent, let's run and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:23:30.442635Z",
     "start_time": "2024-09-11T08:22:56.224422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Assistant): using web_search tool with args: {'queries': ['large language models overview', 'applications of large language models', 'future of large language models', 'impact of large language models on AI']}\n",
      "using extract_content tool with args: {'url': 'https://arxiv.org/abs/2307.06435'}\n",
      "using extract_content tool with args: {'url': 'https://blogs.nvidia.com/blog/what-are-large-language-models-used-for/'}\n",
      "using extract_content tool with args: {'url': 'https://www.techopedia.com/12-practical-large-language-model-llm-applications'}\n",
      "using extract_content tool with args: {'url': 'https://research.aimultiple.com/future-of-large-language-models/'}\n",
      "using extract_content tool with args: {'url': 'https://www.forbes.com/sites/hessiejones/2024/06/21/parsing-the-future-the-promises-and-perils-of-large-language-models/'}\n",
      "Large language models (LLMs) are at the forefront of artificial intelligence, driving innovations in multiple domains including healthcare, finance, education, and entertainment. Here's an overview of what LLMs are, their applications, challenges, and future directions based on recent findings:\n",
      "\n",
      "### Overview of Large Language Models\n",
      "LLMs are deep learning models designed to comprehend and generate human-like text by analyzing vast amounts of data. They excel in tasks like summarization, translation, conversation, and even complex problem-solving. This capability mainly stems from their training on extensive datasets, allowing them to recognize patterns and make contextual predictions.\n",
      "\n",
      "### Key Applications\n",
      "1. **Natural Language Processing**: LLMs improve functionalities in chatbots and AI assistants, enhancing user interaction and engagement.\n",
      "2. **Healthcare**: They aid in analyzing molecular data, generating hypotheses for new treatments, and summarizing medical literature.\n",
      "3. **Software Development**: Tools like **GitHub Copilot** leverage LLMs to assist in writing code, improving efficiency for developers.\n",
      "4. **Content Creation**: They are used in generating creative content such as articles, poems, and even marketing materials.\n",
      "5. **Search Engines**: Enhanced user experience through human-like query responses and more accurate information retrieval.\n",
      "\n",
      "### Current Trends and Future Directions\n",
      "#### 1. **Multimodal Capabilities**:\n",
      "Recent advancements have led to the development of multimodal models (e.g., **GPT-4**, **Google's Gemini**) that can process and generate not only text but also images and audio. This broadens the scope of applications significantly.\n",
      "\n",
      "#### 2. **Ethical Considerations**:\n",
      "As LLMs become more integrated into various sectors, ethical concerns around bias, misinformation, and data privacy remain prevalent. Developers are focusing on techniques such as **retrieval-augmented generation** which combines the LLM’s linguistic capabilities with real-time data querying, improving accuracy and accountability.\n",
      "\n",
      "#### 3. **Domain-Specific Fine-Tuning**:\n",
      "Tailored models aimed at specific industries (e.g., **BloombergGPT** for finance and **Med-PaLM** for healthcare) are being developed to enhance performance and relevance in specialized contexts.\n",
      "\n",
      "#### 4. **Bias Mitigation and Safety**:\n",
      "Companies are increasingly prioritizing ethical AI practices to reduce biases that can be present in training datasets. Implementing strategies for continuous auditing and refinement of output is vital.\n",
      "\n",
      "### Challenges Faced\n",
      "- **Inaccuracy**: LLMs may propagate misinformation and exhibit hallucinations—generating plausible but incorrect information.\n",
      "- **Data Privacy**: Handling sensitive information raises concerns about compliance with regulations and the ethical use of data.\n",
      "- **Resource Intensity**: Training and deploying large models require extensive computational resources, making them accessible primarily to large organizations.\n",
      "\n",
      "### Conclusion\n",
      "The evolution of LLMs represents a significant leap in AI capabilities. As they continue to develop, there is a concerted effort within the tech community to address ethical issues, enhance accuracy, and expand their applicability across different sectors. The future of LLMs appears promising, with a focus on responsible innovation that balances technological advancement with societal impact.\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    web_assistant = WebSearchAgent()\n",
    "    await web_assistant.run()\n",
    "\n",
    "\n",
    "# Run main in a jupyter notebook\n",
    "await main()\n",
    "\n",
    "# Run main in a python script\n",
    "# import asyncio\n",
    "# asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the WebSearchAgent, run the code above and start interacting with it by typing your questions. The agent will use web searches and content extraction to provide answers. Type 'exit' to end the interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Concepts and Best Practices\n",
    "\n",
    "When working with Tools and Agents in Mirascope, consider the following best practices:\n",
    "\n",
    "1. **Error Handling**: Implement robust error handling in your Tools and Agent implementation.\n",
    "2. **Rate Limiting**: Be mindful of rate limits when using external APIs in your Tools.\n",
    "3. **Caching**: Implement caching mechanisms to improve performance and reduce API calls.\n",
    "4. **Testing**: Write unit tests for your Tools and integration tests for your Agents.\n",
    "5. **Modularity**: Design your Tools and Agents to be modular and reusable.\n",
    "6. **Security**: Be cautious about the information you expose through your Tools and Agents.\n",
    "7. **Logging**: Implement logging to track the behavior and performance of your Agents.\n",
    "\n",
    "For more advanced usage, you can explore concepts like:\n",
    "\n",
    "- Multi-agent systems\n",
    "- Tool chaining and composition\n",
    "- Dynamic tool selection\n",
    "- Memory and state management for long-running agents\n",
    "\n",
    "For more techniques and best practices in using Mirascope, refer to the following documentation:\n",
    "\n",
    "- [Dynamic Configuration](https://docs.mirascope.io/latest/learn/dynamic_configuration/)\n",
    "- [Chaining](https://docs.mirascope.io/latest/learn/chaining/)\n",
    "- [Streams](https://docs.mirascope.io/latest/learn/streams/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've explored the basics of creating Tools and implementing Agents in Mirascope. We've built a WebSearchAgent that can perform web searches, extract content from webpages, and use an LLM to generate responses based on the gathered information.\n",
    "\n",
    "This example demonstrates the power and flexibility of Mirascope in building AI applications that combine LLMs with custom tools and logic. As you continue to work with Mirascope, you'll discover more advanced features and patterns that can help you build even more sophisticated AI agents and applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
