{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirascope Quickstart Guide\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "2. [Prompt Templates](#Prompt-Templates)\n",
    "3. [Basic LLM Call](#Basic-LLM-Call)\n",
    "4. [Streaming Responses](#Streaming-Responses)\n",
    "5. [Response Models](#Response-Models)\n",
    "6. [Asynchronous Processing](#Asynchronous-Processing)\n",
    "7. [JSON Mode](#JSON-Mode)\n",
    "8. [Output Parsers](#Output-Parsers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Mirascope supports various LLM providers, including [OpenAI](https://openai.com/), [Anthropic](https://www.anthropic.com/), [Mistral](https://mistral.ai/), [Gemini](https://gemini.google.com), [Groq](https://groq.com/), [Cohere](https://cohere.com/), [LiteLLM](https://www.litellm.ai/), [Azure AI](https://azure.microsoft.com/en-us/solutions/ai), and [Vertex AI](https://cloud.google.com/vertex-ai). For the purposes of this guide, we will be using OpenAI. Let's start by installing Mirascope and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T18:21:04.240185Z",
     "start_time": "2024-09-06T18:21:02.171957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mirascope[anthropic,openai] in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (1.2.0)\r\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[anthropic,openai]) (0.16)\r\n",
      "Requirement already satisfied: jiter>=0.5.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[anthropic,openai]) (0.5.0)\r\n",
      "Requirement already satisfied: pydantic<3.0,>=2.7.4 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[anthropic,openai]) (2.8.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[anthropic,openai]) (4.12.2)\r\n",
      "Collecting anthropic<1.0,>=0.29.0 (from mirascope[anthropic,openai])\r\n",
      "  Downloading anthropic-0.34.2-py3-none-any.whl.metadata (18 kB)\r\n",
      "Requirement already satisfied: openai<2,>=1.6.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[anthropic,openai]) (1.43.0)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from anthropic<1.0,>=0.29.0->mirascope[anthropic,openai]) (4.4.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from anthropic<1.0,>=0.29.0->mirascope[anthropic,openai]) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from anthropic<1.0,>=0.29.0->mirascope[anthropic,openai]) (0.27.2)\r\n",
      "Requirement already satisfied: sniffio in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from anthropic<1.0,>=0.29.0->mirascope[anthropic,openai]) (1.3.1)\r\n",
      "Collecting tokenizers>=0.13.0 (from anthropic<1.0,>=0.29.0->mirascope[anthropic,openai])\r\n",
      "  Downloading tokenizers-0.20.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: tqdm>4 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from openai<2,>=1.6.0->mirascope[anthropic,openai]) (4.66.5)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from pydantic<3.0,>=2.7.4->mirascope[anthropic,openai]) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from pydantic<3.0,>=2.7.4->mirascope[anthropic,openai]) (2.20.1)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from anyio<5,>=3.5.0->anthropic<1.0,>=0.29.0->mirascope[anthropic,openai]) (3.8)\r\n",
      "Requirement already satisfied: certifi in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from httpx<1,>=0.23.0->anthropic<1.0,>=0.29.0->mirascope[anthropic,openai]) (2024.8.30)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from httpx<1,>=0.23.0->anthropic<1.0,>=0.29.0->mirascope[anthropic,openai]) (1.0.5)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<1.0,>=0.29.0->mirascope[anthropic,openai]) (0.14.0)\r\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic,openai])\r\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic,openai])\r\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic,openai])\r\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic,openai]) (24.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic,openai]) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic,openai]) (2.32.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic,openai]) (3.3.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic,openai]) (2.2.2)\r\n",
      "Downloading anthropic-0.34.2-py3-none-any.whl (891 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.9/891.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tokenizers-0.20.0-cp312-cp312-macosx_11_0_arm64.whl (2.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\r\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\r\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\r\n",
      "Installing collected packages: fsspec, filelock, huggingface-hub, tokenizers, anthropic\r\n",
      "Successfully installed anthropic-0.34.2 filelock-3.15.4 fsspec-2024.9.0 huggingface-hub-0.24.6 tokenizers-0.20.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"mirascope[openai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command installs Mirascope along with the necessary packages for the OpenAI integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "# Set the appropriate API key for the provider you're using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "Prompt templates in Mirascope allow you to create dynamic and reusable prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mirascope.core import prompt_template\n",
    "\n",
    "\n",
    "@prompt_template(\"What is the capital of {country}?\")\n",
    "def get_capital_prompt(country: str): ...\n",
    "\n",
    "\n",
    "print(get_capital_prompt(\"Japan\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "1. The `@prompt_template` decorator defines the structure of the prompt.\n",
    "2. `{country}` is a placeholder that gets replaced with the matching argument passed to the function.\n",
    "\n",
    "Here's an example of a multi-line prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@prompt_template(\n",
    "    \"\"\"\n",
    "    SYSTEM:\n",
    "    You are a knowledgeable assistant specializing in world geography.\n",
    "    Your task is to provide detailed information about the capital cities of various countries.\n",
    "    \n",
    "    USER:\n",
    "    Please provide the following information about the capital of {country}:\n",
    "    1. The name of the capital city\n",
    "    2. Its population (approximate is fine)\n",
    "    3. Three major landmarks or attractions\n",
    "    \"\"\"\n",
    ")\n",
    "def get_capital_info(country: str): ...\n",
    "\n",
    "\n",
    "capital_info = get_capital_info(\"France\")\n",
    "print(capital_info.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates:\n",
    "1. A multi-line prompt template with proper indentation.\n",
    "2. Use of \"SYSTEM\" and \"USER\" keywords to define messages with different roles.\n",
    "3. A complex, multi-part query within the USER message.\n",
    "4. Use of a variable (`{country}`) to create a dynamic prompt.\n",
    "\n",
    "Prompt templates like this improve code readability, maintain consistency in prompts, and make it easier to experiment with and modify prompts for effective prompt engineering. They are particularly useful for complex queries that require detailed instructions or context.\n",
    "\n",
    "For more detailed information on prompt templates and advanced usage, check out our [documentation on Prompts](https://docs.mirascope.io/learn/prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LLM Call\n",
    "\n",
    "The `call` decorator transforms functions with prompt templates into LLM API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mirascope.core import openai\n",
    "\n",
    "# import provider-specific modules to use those providers (e.g. anthropic, gemini, etc.)\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template(\"What is the capital of {country}?\")\n",
    "def get_capital(country: str): ...\n",
    "\n",
    "\n",
    "response = get_capital(\"Japan\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "For more advanced usage of LLM calls, refer to our [documentation on Calls](https://docs.mirascope.io/learn/calls)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "\n",
    "Streaming allows you to process LLM responses in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@openai.call(\"gpt-4o-mini\", stream=True)\n",
    "@prompt_template(\"Provide a brief description of {city}.\")\n",
    "def stream_city_info(city: str): ...\n",
    "\n",
    "\n",
    "for chunk, _ in stream_city_info(\"Tokyo\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "For more information on streaming, including advanced techniques for processing streamed content, see our [documentation on Streams](https://docs.mirascope.io/learn/streams)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Models\n",
    "\n",
    "Response models allow you to structure and validate the output from LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T18:17:06.472382Z",
     "start_time": "2024-09-06T18:17:05.902653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city='Paris' country='France'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Capital(BaseModel):\n",
    "    city: str\n",
    "    country: str\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\", response_model=Capital)\n",
    "@prompt_template(\"{query}\")\n",
    "def extract_capital(query: str): ...\n",
    "\n",
    "\n",
    "capital = extract_capital(\"The capital of France is Paris\")\n",
    "print(capital)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "For more details on response models, including advanced validation techniques, check our [documentation on Response Models](https://docs.mirascope.io/learn/response-models)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Processing\n",
    "\n",
    "Mirascope supports asynchronous processing for efficient parallel execution of multiple LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T18:15:04.233526Z",
     "start_time": "2024-09-06T18:15:03.521218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris\n",
      "The capital of Japan is Tokyo\n",
      "The capital of Brazil is Brasilia\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\", response_model=Capital)\n",
    "@prompt_template(\"What is the capital of {country}?\")\n",
    "async def get_capital_async(country: str): ...\n",
    "\n",
    "\n",
    "async def main():\n",
    "    countries = [\"France\", \"Japan\", \"Brazil\"]\n",
    "    tasks = [get_capital_async(country) for country in countries]\n",
    "    capitals = await asyncio.gather(*tasks)\n",
    "    for capital in capitals:\n",
    "        print(f\"The capital of {capital.country} is {capital.city}\")\n",
    "\n",
    "\n",
    "# await main() when running in a Jupyter notebook\n",
    "await main()\n",
    "\n",
    "# asyncio.run(main()) when running in a Python script\n",
    "# asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "For more advanced usage of asynchronous processing, including async streaming, see our [documentation on Async](https://docs.mirascope.io/learn/async)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Mode\n",
    "\n",
    "JSON mode allows you to directly parse LLM outputs as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@openai.call(\"gpt-4o-mini\", json_mode=True)\n",
    "@prompt_template(\"Provide information about {city} in JSON format\")\n",
    "def city_info(city: str): ...\n",
    "\n",
    "\n",
    "response = city_info(\"Tokyo\")\n",
    "print(response.content)  # This will be a JSON-formatted string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that not all providers have an explicit JSON mode. For those providers, we attempt to instruct the model to provide JSON; however, there is no guarantee that it will output only JSON (it may start with some text like \"Here is the JSON: ...\"). This is where [Output Parsers](https://docs.mirascope.io/learn/output-parsers) can be useful.\n",
    "\n",
    "For more information on JSON mode, refer to our [documentation on JSON Mode](https://docs.mirascope.io/learn/json-mode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Output parsers allow you to process LLM responses in custom formats. They are particularly useful when working with JSON outputs, especially for providers like Anthropic that don't have a strict JSON mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T18:25:27.406027Z",
     "start_time": "2024-09-06T18:25:26.809910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mirascope[anthropic] in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (1.2.0)\r\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[anthropic]) (0.16)\r\n",
      "Requirement already satisfied: jiter>=0.5.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[anthropic]) (0.5.0)\r\n",
      "Requirement already satisfied: pydantic<3.0,>=2.7.4 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[anthropic]) (2.8.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[anthropic]) (4.12.2)\r\n",
      "Requirement already satisfied: anthropic<1.0,>=0.29.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[anthropic]) (0.34.2)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from anthropic<1.0,>=0.29.0->mirascope[anthropic]) (4.4.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from anthropic<1.0,>=0.29.0->mirascope[anthropic]) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from anthropic<1.0,>=0.29.0->mirascope[anthropic]) (0.27.2)\r\n",
      "Requirement already satisfied: sniffio in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from anthropic<1.0,>=0.29.0->mirascope[anthropic]) (1.3.1)\r\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from anthropic<1.0,>=0.29.0->mirascope[anthropic]) (0.20.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from pydantic<3.0,>=2.7.4->mirascope[anthropic]) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from pydantic<3.0,>=2.7.4->mirascope[anthropic]) (2.20.1)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from anyio<5,>=3.5.0->anthropic<1.0,>=0.29.0->mirascope[anthropic]) (3.8)\r\n",
      "Requirement already satisfied: certifi in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from httpx<1,>=0.23.0->anthropic<1.0,>=0.29.0->mirascope[anthropic]) (2024.8.30)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from httpx<1,>=0.23.0->anthropic<1.0,>=0.29.0->mirascope[anthropic]) (1.0.5)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<1.0,>=0.29.0->mirascope[anthropic]) (0.14.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic]) (0.24.6)\r\n",
      "Requirement already satisfied: filelock in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic]) (3.15.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic]) (2024.9.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic]) (24.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic]) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic]) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic]) (4.66.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic]) (3.3.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1.0,>=0.29.0->mirascope[anthropic]) (2.2.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install \"mirascope[anthropic]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T18:25:37.890305Z",
     "start_time": "2024-09-06T18:25:36.354596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capital': 'Paris', 'country': 'France'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from mirascope.core import anthropic\n",
    "\n",
    "\n",
    "def only_json(response: anthropic.AnthropicCallResponse) -> str:\n",
    "    json_start = response.content.index(\"{\")\n",
    "    json_end = response.content.rfind(\"}\")\n",
    "    return response.content[json_start : json_end + 1]\n",
    "\n",
    "\n",
    "@anthropic.call(\"claude-3-5-sonnet-20240620\", json_mode=True, output_parser=only_json)\n",
    "@prompt_template(\"Extract {fields} from the following text: {text}\")\n",
    "def json_extraction(text: str, fields: list[str]): ...\n",
    "\n",
    "\n",
    "json_response = json_extraction(\n",
    "    text=\"The capital of France is Paris\",\n",
    "    fields=[\"capital\", \"country\"],\n",
    ")\n",
    "print(json.loads(json_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "For more information on output parsers, see our [documentation on Output Parsers](https://docs.mirascope.io/learn/output-parsers)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our Quickstart Guide to Mirascope. We've covered the main features of the library, including prompt templates, basic calls, streaming, response models, asynchronous processing, JSON mode, and output parsers. Each of these features can be combined and customized to create powerful, flexible AI applications.\n",
    "\n",
    "For more detailed information on each of these topics and advanced usage, including dynamic configuration and chaining, please refer to our comprehensive [Learn documentation](https://docs.mirascope.io/learn)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
