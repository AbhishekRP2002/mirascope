{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirascope Quickstart Guide\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "2. [Prompt Templates](#Prompt-Templates)\n",
    "3. [Basic LLM Call](#Basic-LLM-Call)\n",
    "4. [Streaming Responses](#Streaming-Responses)\n",
    "5. [Response Models](#Response-Models)\n",
    "6. [Asynchronous Processing](#Asynchronous-Processing)\n",
    "7. [JSON Mode](#JSON-Mode)\n",
    "8. [Output Parsers](#Output-Parsers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Mirascope supports various LLM providers, including [OpenAI](https://openai.com/), [Anthropic](https://www.anthropic.com/), [Mistral](https://mistral.ai/), [Gemini](https://gemini.google.com), [Groq](https://groq.com/), [Cohere](https://cohere.com/), [LiteLLM](https://www.litellm.ai/), [Azure AI](https://azure.microsoft.com/en-us/solutions/ai), and [Vertex AI](https://cloud.google.com/vertex-ai). For the purposes of this guide, we will be using OpenAI. Let's start by installing Mirascope and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T08:29:09.802769Z",
     "start_time": "2024-09-09T08:29:09.313470Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mirascope[openai] in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[openai]) (0.16)\n",
      "Requirement already satisfied: jiter>=0.5.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[openai]) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3.0,>=2.7.4 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[openai]) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[openai]) (4.12.2)\n",
      "Requirement already satisfied: openai<2,>=1.6.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from mirascope[openai]) (1.43.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from openai<2,>=1.6.0->mirascope[openai]) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from openai<2,>=1.6.0->mirascope[openai]) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from openai<2,>=1.6.0->mirascope[openai]) (0.27.2)\n",
      "Requirement already satisfied: sniffio in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from openai<2,>=1.6.0->mirascope[openai]) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from openai<2,>=1.6.0->mirascope[openai]) (4.66.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from pydantic<3.0,>=2.7.4->mirascope[openai]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from pydantic<3.0,>=2.7.4->mirascope[openai]) (2.20.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<2,>=1.6.0->mirascope[openai]) (3.8)\n",
      "Requirement already satisfied: certifi in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2,>=1.6.0->mirascope[openai]) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2,>=1.6.0->mirascope[openai]) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/koudai/PycharmProjects/mirascope/.venv_notebook/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2,>=1.6.0->mirascope[openai]) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"mirascope[openai]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command installs Mirascope along with the necessary packages for the OpenAI integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "# Set the appropriate API key for the provider you're using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "Prompt templates in Mirascope allow you to create dynamic and reusable prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T08:51:48.023644Z",
     "start_time": "2024-09-09T08:51:48.015880Z"
    },
    "jupyter": {
     "source_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BaseMessageParam(role='user', content='What is the capital of Japan?')]\n"
     ]
    }
   ],
   "source": [
    "from mirascope.core import prompt_template\n",
    "\n",
    "\n",
    "@prompt_template(\"What is the capital of {country}?\")\n",
    "def get_capital_prompt(country: str): ...\n",
    "\n",
    "\n",
    "print(get_capital_prompt(\"Japan\"))\n",
    "# [BaseMessageParam(role='user', content='What is the capital of Japan?')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "1. We import the `prompt_template` decorator from Mirascope.\n",
    "2. We define a function `get_capital_prompt` and decorate it with `@prompt_template`.\n",
    "3. The `{country}` placeholder in the template will be replaced with the actual argument passed to the function.\n",
    "4. When we call `get_capital_prompt(\"Japan\")`, it returns a list containing a `BaseMessageParam` object with the role 'user' and the content \"What is the capital of Japan?\"\n",
    "\n",
    "Prompt templates are particularly useful for creating consistent prompts across multiple calls and for easily modifying prompt structures. Here's a more complex example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@prompt_template(\n",
    "    \"\"\"\n",
    "    SYSTEM:\n",
    "    You are a knowledgeable assistant specializing in world geography.\n",
    "    Your task is to provide detailed information about the capital cities of various countries.\n",
    "    \n",
    "    USER:\n",
    "    Please provide the following information about the capital of {country}:\n",
    "    1. The name of the capital city\n",
    "    2. Its population (approximate is fine)\n",
    "    3. Three major landmarks or attractions\n",
    "    \"\"\"\n",
    ")\n",
    "def get_capital_info(country: str): ...\n",
    "\n",
    "\n",
    "print(get_capital_info(\"France\"))\n",
    "# [BaseMessageParam(role='system', content='You are a knowledgeable assistant specializing in world geography.\\nYour task is to provide detailed information about the capital cities of various countries.'),\n",
    "#  BaseMessageParam(role='user', content='Please provide the following information about the capital of France:\\n1. The name of the capital city\\n2. Its population (approximate is fine)\\n3. Three major landmarks or attractions')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This more advanced example demonstrates:\n",
    "1. A multi-line prompt with distinct SYSTEM and USER roles, useful for chat-based LLMs.\n",
    "2. Structured queries within the prompt, allowing for more detailed and specific responses.\n",
    "3. Proper indentation and formatting for readability and correct parsing.\n",
    "4. The function now returns a list of `BaseMessageParam` objects, one for each role (SYSTEM and USER) in the prompt.\n",
    "\n",
    "Prompt templates improve code readability, maintain consistency, and facilitate experimentation with different prompt structures. They also provide a structured way to work with different message roles in chat-based LLM interactions.\n",
    "\n",
    "For more advanced techniques, including handling multiple variables and conditional prompts, refer to our [documentation on Prompts](https://docs.mirascope.io/learn/prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LLM Call\n",
    "\n",
    "The `call` decorator in Mirascope transforms functions with prompt templates into LLM API calls. This allows you to seamlessly integrate LLM interactions into your Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T01:11:02.182404Z",
     "start_time": "2024-09-07T01:11:01.174057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Japan is Tokyo.\n"
     ]
    }
   ],
   "source": [
    "from mirascope.core import openai\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "@prompt_template(\"What is the capital of {country}?\")\n",
    "def get_capital(country: str): ...\n",
    "\n",
    "\n",
    "response = get_capital(\"Japan\")\n",
    "print(response.content)\n",
    "# The capital of Japan is Tokyo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "1. We import the `openai` module from Mirascope, which provides the `call` decorator.\n",
    "2. The `@openai.call(\"gpt-4o-mini\")` decorator specifies which OpenAI model to use.\n",
    "3. We combine the `call` decorator with our previous `prompt_template`.\n",
    "4. The `...` in the function body indicates that we don't need to implement the function ourselves; Mirascope handles the LLM interaction.\n",
    "5. When we call `get_capital(\"Japan\")`, it sends a request to the OpenAI API and returns the response.\n",
    "6. We print the `content` of the response, which contains the LLM's answer.\n",
    "\n",
    "This approach allows you to use LLMs as if they were regular Python functions, making it easy to integrate AI capabilities into your applications. For more advanced usage, including controlling model parameters and handling errors, see our [documentation on Calls](https://docs.mirascope.io/learn/calls)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "\n",
    "Streaming allows you to process LLM responses in real-time, which is particularly useful for long-form content generation or when you want to provide immediate feedback to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T01:11:10.122995Z",
     "start_time": "2024-09-07T01:11:08.330200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokyo, the capital of Japan, is a vibrant and bustling metropolis known for its unique blend of traditional culture and modern innovation. With a population of over 13 million people, it is one of the most populous cities in the world. Tokyo is famous for its towering skyscrapers, historic temples, and diverse neighborhoods, ranging from the fashion-forward streets of Harajuku to the historic Asakusa district. The city offers a rich culinary scene, world-class shopping, and a lively arts and entertainment scene. Its efficient public transportation system makes it easy to explore, while its blend of old and new provides a captivating experience for both residents and visitors."
     ]
    }
   ],
   "source": [
    "@openai.call(\"gpt-4o-mini\", stream=True)\n",
    "@prompt_template(\"Provide a brief description of {city}.\")\n",
    "def stream_city_info(city: str): ...\n",
    "\n",
    "\n",
    "for chunk, _ in stream_city_info(\"Tokyo\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "# Tokyo, the capital of Japan, is a vibrant and bustling metropolis known for its unique blend of traditional culture and modern innovation. With a population of over 13 million people, it is one of the most populous cities in the world. Tokyo is famous for its towering skyscrapers, historic temples, and diverse neighborhoods, ranging from the fashion-forward streets of Harajuku to the historic Asakusa district. The city offers a rich culinary scene, world-class shopping, and a lively arts and entertainment scene. Its efficient public transportation system makes it easy to explore, while its blend of old and new provides a captivating experience for both residents and visito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what's happening in this streaming example:\n",
    "1. We use the `stream=True` parameter in the `@openai.call` decorator to enable streaming.\n",
    "2. The function returns an iterator that yields chunks of the response as they become available.\n",
    "3. We iterate over the chunks, printing each one immediately.\n",
    "4. The `end=\"\"` and `flush=True` parameters in the print function ensure that the output is displayed in real-time without line breaks.\n",
    "\n",
    "Streaming is beneficial for:\n",
    "- Providing immediate feedback to users\n",
    "- Processing very long responses efficiently\n",
    "- Implementing typewriter-like effects in user interfaces\n",
    "\n",
    "For more advanced streaming techniques, including error handling and processing streamed content, refer to our [documentation on Streams](https://docs.mirascope.io/learn/streams)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Models\n",
    "\n",
    "Response models in Mirascope allow you to structure and validate the output from LLMs. This feature is particularly useful when you need to ensure that the LLM's response adheres to a specific format or contains certain fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T01:11:49.985674Z",
     "start_time": "2024-09-07T01:11:47.177952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city='Paris' country='France'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Capital(BaseModel):\n",
    "    city: str\n",
    "    country: str\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\", response_model=Capital)\n",
    "@prompt_template(\"{query}\")\n",
    "def extract_capital(query: str): ...\n",
    "\n",
    "\n",
    "capital = extract_capital(\"The capital of France is Paris\")\n",
    "print(capital)\n",
    "# city='Paris' country='France'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on response models, including advanced validation techniques, check our [documentation on Response Models](https://docs.mirascope.io/learn/response-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Processing\n",
    "\n",
    "Mirascope supports asynchronous processing, allowing for efficient parallel execution of multiple LLM calls. This is particularly useful when you need to make many LLM calls concurrently or when working with asynchronous web frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T01:11:57.536376Z",
     "start_time": "2024-09-07T01:11:56.163833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris\n",
      "The capital of Japan is Tokyo\n",
      "The capital of Brazil is Brazil\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\", response_model=Capital)\n",
    "@prompt_template(\"What is the capital of {country}?\")\n",
    "async def get_capital_async(country: str): ...\n",
    "\n",
    "\n",
    "async def main():\n",
    "    countries = [\"France\", \"Japan\", \"Brazil\"]\n",
    "    tasks = [get_capital_async(country) for country in countries]\n",
    "    capitals = await asyncio.gather(*tasks)\n",
    "    for capital in capitals:\n",
    "        print(f\"The capital of {capital.country} is {capital.city}\")\n",
    "\n",
    "\n",
    "# await main() when running in a Jupyter notebook\n",
    "await main()\n",
    "\n",
    "# The capital of France is Paris\n",
    "# The capital of Japan is Tokyo\n",
    "# The capital of Brazil is Brazil\n",
    "\n",
    "# asyncio.run(main()) when running in a Python script\n",
    "# asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This asynchronous example demonstrates:\n",
    "1. An async version of our `get_capital` function, defined with `async def`.\n",
    "2. Use of `asyncio.gather()` to run multiple async tasks concurrently.\n",
    "3. Processing of results as they become available.\n",
    "\n",
    "Asynchronous processing offers several advantages:\n",
    "- Improved performance when making multiple LLM calls\n",
    "- Better resource utilization\n",
    "- Compatibility with async web frameworks like FastAPI or aiohttp\n",
    "\n",
    "For more advanced asynchronous techniques, including error handling and async streaming, refer to our [documentation on Async](https://docs.mirascope.io/learn/async)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Mode\n",
    "\n",
    "JSON mode allows you to directly parse LLM outputs as JSON. This is particularly useful when you need structured data from your LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T01:12:28.598246Z",
     "start_time": "2024-09-07T01:12:24.723056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"city\": \"Tokyo\",\n",
      "  \"country\": \"Japan\",\n",
      "  \"population\": 13929286,\n",
      "  \"area_km2\": 2191,\n",
      "  \"famous_landmarks\": [\n",
      "    {\n",
      "      \"name\": \"Tokyo Tower\",\n",
      "      \"type\": \"Observation Tower\",\n",
      "      \"height_m\": 333\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Senso-ji\",\n",
      "      \"type\": \"Temple\",\n",
      "      \"region\": \"Asakusa\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Shibuya Crossing\",\n",
      "      \"type\": \"Intersection\",\n",
      "      \"description\": \"Famous pedestrian scramble\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Tokyo Skytree\",\n",
      "      \"type\": \"Broadcasting Tower\",\n",
      "      \"height_m\": 634\n",
      "    }\n",
      "  ],\n",
      "  \"public_transportation\": {\n",
      "    \"subway_lines\": 13,\n",
      "    \"bus_routes\": 1000,\n",
      "    \"railway_network\": \"Tokyo Metro and JR East\"\n",
      "  },\n",
      "  \"cuisine\": [\n",
      "    {\n",
      "      \"name\": \"Sushi\",\n",
      "      \"description\": \"Vinegared rice with raw fish\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Ramen\",\n",
      "      \"description\": \"Japanese noodle soup\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Tempura\",\n",
      "      \"description\": \"Batter-fried seafood and vegetables\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "@openai.call(\"gpt-4o-mini\", json_mode=True)\n",
    "@prompt_template(\"Provide information about {city} in JSON format\")\n",
    "def city_info(city: str): ...\n",
    "\n",
    "\n",
    "response = city_info(\"Tokyo\")\n",
    "print(response.content)  # This will be a JSON-formatted string\n",
    "# {\n",
    "#  \"city\": \"Tokyo\",\n",
    "#  \"country\": \"Japan\",\n",
    "#  \"population\": 13929286,\n",
    "#  \"area_km2\": 2191,\n",
    "#  \"famous_landmarks\": [\n",
    "#    {\n",
    "#      \"name\": \"Tokyo Tower\",\n",
    "#      \"type\": \"Observation Tower\",\n",
    "#      \"height_m\": 333\n",
    "#    },\n",
    "#    {\n",
    "#      \"name\": \"Senso-ji\",\n",
    "#      \"type\": \"Temple\",\n",
    "#      \"region\": \"Asakusa\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"name\": \"Shibuya Crossing\",\n",
    "#      \"type\": \"Intersection\",\n",
    "#      \"description\": \"Famous pedestrian scramble\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"name\": \"Tokyo Skytree\",\n",
    "#      \"type\": \"Broadcasting Tower\",\n",
    "#      \"height_m\": 634\n",
    "#    }\n",
    "#  ],\n",
    "#  \"public_transportation\": {\n",
    "#    \"subway_lines\": 13,\n",
    "#    \"bus_routes\": 1000,\n",
    "#    \"railway_network\": \"Tokyo Metro and JR East\"\n",
    "#  },\n",
    "#  \"cuisine\": [\n",
    "#    {\n",
    "#      \"name\": \"Sushi\",\n",
    "#      \"description\": \"Vinegared rice with raw fish\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"name\": \"Ramen\",\n",
    "#      \"description\": \"Japanese noodle soup\"\n",
    "#    },\n",
    "#    {\n",
    "#      \"name\": \"Tempura\",\n",
    "#      \"description\": \"Batter-fried seafood and vegetables\"\n",
    "#    }\n",
    "#  ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "1. We use `json_mode=True` in the `@openai.call` decorator to instruct the LLM to return JSON.\n",
    "2. The LLM's response will be a valid JSON string, which can be easily parsed into a Python dictionary or object.\n",
    "\n",
    "JSON mode is beneficial for:\n",
    "- Ensuring structured outputs from LLMs\n",
    "- Easy integration with data processing pipelines\n",
    "- Creating APIs that return JSON data\n",
    "\n",
    "Note that not all providers have an explicit JSON mode. For those providers, we attempt to instruct the model to provide JSON; however, there is no guarantee that it will output only JSON (it may start with some text like \"Here is the JSON: ...\"). This is where Output Parsers can be useful.\n",
    "\n",
    "For more information on JSON mode and its limitations with different providers, refer to our [documentation on JSON Mode](https://docs.mirascope.io/learn/json-mode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Output parsers allow you to process LLM responses in custom formats. They are particularly useful when working with JSON outputs, especially for providers like Anthropic that don't have a strict JSON mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"mirascope[anthropic]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T01:13:25.260772Z",
     "start_time": "2024-09-07T01:13:23.864236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capital': 'Paris', 'country': 'France'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from mirascope.core import anthropic\n",
    "\n",
    "\n",
    "def only_json(response: anthropic.AnthropicCallResponse) -> str:\n",
    "    json_start = response.content.index(\"{\")\n",
    "    json_end = response.content.rfind(\"}\")\n",
    "    return response.content[json_start : json_end + 1]\n",
    "\n",
    "\n",
    "@anthropic.call(\"claude-3-5-sonnet-20240620\", json_mode=True, output_parser=only_json)\n",
    "@prompt_template(\"Extract {fields} from the following text: {text}\")\n",
    "def json_extraction(text: str, fields: list[str]): ...\n",
    "\n",
    "\n",
    "json_response = json_extraction(\n",
    "    text=\"The capital of France is Paris\",\n",
    "    fields=[\"capital\", \"country\"],\n",
    ")\n",
    "print(json.loads(json_response))\n",
    "# {'capital': 'Paris', 'country': 'France'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "1. We define a custom `only_json` parser that extracts the JSON portion from the response.\n",
    "2. We use both `json_mode=True` and the custom output parser to ensure we get clean JSON output.\n",
    "3. The `json_extraction` function demonstrates how to combine JSON mode with a custom parser.\n",
    "\n",
    "Output parsers are useful for:\n",
    "- Extracting specific formats or data structures from LLM responses\n",
    "- Cleaning and standardizing LLM outputs\n",
    "- Implementing custom post-processing logic\n",
    "\n",
    "For more information on output parsers and advanced usage scenarios, see our [documentation on Output Parsers](https://docs.mirascope.io/learn/output-parsers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our Quickstart Guide to Mirascope. We've covered the main features of the library, including prompt templates, basic calls, streaming, response models, asynchronous processing, JSON mode, and output parsers. Each of these features can be combined and customized to create powerful, flexible AI applications.\n",
    "\n",
    "For more detailed information on each of these topics and advanced usage, including dynamic configuration and chaining, please refer to our comprehensive [Learn documentation](https://docs.mirascope.io/learn)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
